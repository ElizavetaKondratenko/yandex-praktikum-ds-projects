{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38f86013",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Описание-проекта\" data-toc-modified-id=\"Описание-проекта-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Описание проекта</a></span></li><li><span><a href=\"#Описание-и-предобработка-данных\" data-toc-modified-id=\"Описание-и-предобработка-данных-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Описание и предобработка данных</a></span><ul class=\"toc-item\"><li><span><a href=\"#Загрузка-и-первичный-анализ-данных\" data-toc-modified-id=\"Загрузка-и-первичный-анализ-данных-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Загрузка и первичный анализ данных</a></span></li><li><span><a href=\"#Лемматизация-текстов\" data-toc-modified-id=\"Лемматизация-текстов-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Лемматизация текстов</a></span></li><li><span><a href=\"#Отчистка-текста\" data-toc-modified-id=\"Отчистка-текста-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Отчистка текста</a></span></li><li><span><a href=\"#Деление-данных-на-трейн-и-тест\" data-toc-modified-id=\"Деление-данных-на-трейн-и-тест-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Деление данных на трейн и тест</a></span></li></ul></li><li><span><a href=\"#Построение-моделей\" data-toc-modified-id=\"Построение-моделей-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Построение моделей</a></span><ul class=\"toc-item\"><li><span><a href=\"#Логистическая-регрессия\" data-toc-modified-id=\"Логистическая-регрессия-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Логистическая регрессия</a></span></li><li><span><a href=\"#Случайный-лес\" data-toc-modified-id=\"Случайный-лес-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Случайный лес</a></span></li><li><span><a href=\"#CatBoost-классификация\" data-toc-modified-id=\"CatBoost-классификация-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>CatBoost классификация</a></span></li></ul></li><li><span><a href=\"#Итоговые-выводы\" data-toc-modified-id=\"Итоговые-выводы-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Итоговые выводы</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf689e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Elizaveta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Elizaveta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Elizaveta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95399e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cbe79d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1a7544",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e80fa7",
   "metadata": {},
   "source": [
    "## Описание проекта"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ca637",
   "metadata": {},
   "source": [
    "**Заказчик** - интернет-магазин «Викишоп», который планирует запустить новый сервис. Благодаря данному сервису пользователи смогут редактировать и дополнять описания товаров подобно тому, как это делается в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию - модель, которая будет классифицировать комментарии на позитивные и негативные. При этом нужно достичь значение метрики качества F1 не меньше 0.75.\n",
    "\n",
    "**Исходные данные** - набор данных с разметкой о токсичности правок."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2eee96",
   "metadata": {},
   "source": [
    "## Описание и предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbfbb3c",
   "metadata": {},
   "source": [
    "### Загрузка и первичный анализ данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07112534",
   "metadata": {},
   "source": [
    "Начнем работу с загрузки данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bda6aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv('/datasets/toxic_comments.csv', on_bad_lines='skip')\n",
    "except:\n",
    "    data = pd.read_csv('toxic_comments.csv', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b3179",
   "metadata": {},
   "source": [
    "Также перед началом работы выведем основную информацию об исходных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c495f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3473981a",
   "metadata": {},
   "source": [
    "Исходный набор данных состоит из почти 160 тыс. наблюдений. Информативных столбца два: сам текст описания или правки, а также пометка о том, был ли отнесен данных текст к категории \"токсичный\" или нет. Пропусков в данных не обнаружено, тип переменных соответствует их смыслу.\n",
    "\n",
    "Дополнительно рассмотрим переменную 'toxic' на распределение классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e1bc4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='toxic', ylabel='count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = 'toxic', data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06487b0",
   "metadata": {},
   "source": [
    "Как можно заметить превалирует нулевой класс, т.е. те комментарии, которые не были отмечены как токсичные. Это будет необходимо учесть при разбиении на подвыборки и в процессе обучения моделей. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087df478",
   "metadata": {},
   "source": [
    "### Лемматизация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0a264e",
   "metadata": {},
   "source": [
    "Начнем предобработку текстов. Для начала проведем лемматизацию. Поскольку используемый нами лемматайзер наиболее эффективно обрабатывает слова, зная их часть речи, дополнительно создадим функцию, которая присвоит каждому слову в тексте соответствующую ему часть речи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "111916f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b9afb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f60888",
   "metadata": {},
   "source": [
    "Также создадим функцию, которая будет проводить процедуру лемматизации для всех наблюдений. На вход она получает строку, после чего делит текст пословно и определяет часть речи каждого слова. После этого, зная слово и его часть речи, лемматайзер будет приводить слово к исходной форме. По итогу функция возвращает строку, в которой все слова приведены к исходной форме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7af96ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemms_creating(row):\n",
    "    pos_tagged = nltk.pos_tag(nltk.word_tokenize(row['text'])) \n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "    \n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            # if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:       \n",
    "            # else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "        \n",
    "    lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5df85a9",
   "metadata": {},
   "source": [
    "Применим получившиеся функции к данным и выведем полученный результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4350e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text']= data.apply(lemms_creating, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69b275dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits make under my userna...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww ! He match this background colour I 'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man , I 'm really not try to edit war . It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>`` More I ca n't make any real suggestion on i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You , sir , be my hero . Any chance you rememb...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation Why the edits make under my userna...      0\n",
       "1           1  D'aww ! He match this background colour I 'm s...      0\n",
       "2           2  Hey man , I 'm really not try to edit war . It...      0\n",
       "3           3  `` More I ca n't make any real suggestion on i...      0\n",
       "4           4  You , sir , be my hero . Any chance you rememb...      0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00c0e43",
   "metadata": {},
   "source": [
    "Как можно заметить, преобразование в большинстве своем прошло успешно - не были обработан только слова с ошибками  и опечатки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb65f05",
   "metadata": {},
   "source": [
    "### Отчистка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd64ded",
   "metadata": {},
   "source": [
    "Теперь проведем чистку текста от ненужных символов, т.е. оставим только буквы и пробелы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8bc73aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(row):\n",
    "    text_new = re.sub(r'[^a-zA-Z ]', ' ', row['text'])\n",
    "    text_new = text_new.split()\n",
    "    text_new = \" \".join(text_new)\n",
    "    return text_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ac46109",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data.apply(clear_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "052e791f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits make under my userna...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D aww He match this background colour I m seem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man I m really not try to edit war It s ju...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>More I ca n t make any real suggestion on impr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You sir be my hero Any chance you remember wha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation Why the edits make under my userna...      0\n",
       "1           1  D aww He match this background colour I m seem...      0\n",
       "2           2  Hey man I m really not try to edit war It s ju...      0\n",
       "3           3  More I ca n t make any real suggestion on impr...      0\n",
       "4           4  You sir be my hero Any chance you remember wha...      0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433b51cb",
   "metadata": {},
   "source": [
    "Чистка текста также прошла успешно - ненужные и неинформативные символы были удалены."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27255fb",
   "metadata": {},
   "source": [
    "### Деление данных на трейн и тест"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27ef493",
   "metadata": {},
   "source": [
    "На текущий момент предобработка закончена. Теперь поделим наши обработанные данные на тренировочные и тестовые."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a3250cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['text']].copy()\n",
    "y = data[['toxic']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0317370",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    stratify = y,\n",
    "                                                    shuffle = True,\n",
    "                                                    random_state = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dc3f821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129813</th>\n",
       "      <td>From the lead onwards the page refers to chip ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116545</th>\n",
       "      <td>The English language literature do talk a lot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90600</th>\n",
       "      <td>Being intimidate at the poll place get my clea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84698</th>\n",
       "      <td>PICTURE YOUR MOTHER WITH HER WRINKLED UP ASS I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101620</th>\n",
       "      <td>I ll be go back myself to look at the date est...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text\n",
       "129813  From the lead onwards the page refers to chip ...\n",
       "116545  The English language literature do talk a lot ...\n",
       "90600   Being intimidate at the poll place get my clea...\n",
       "84698   PICTURE YOUR MOTHER WITH HER WRINKLED UP ASS I...\n",
       "101620  I ll be go back myself to look at the date est..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1239712f",
   "metadata": {},
   "source": [
    "Мы изучили исходные данные, обработали текст: перевели все слова к исходным формам, а также почистили их от ненужных символов. Теперь мы можем приступить к этапу моделирования. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8374418c",
   "metadata": {},
   "source": [
    "## Построение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53ecb51",
   "metadata": {},
   "source": [
    "В данном проекте мы рассмотрим три группы моделей: логистическую регрессию, случайный лес и CatBoost классификатор. Для каждой из моделей мы попытаемся подобрать наилучшие параметры основываясь на значении метрики качества F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6da9ab5",
   "metadata": {},
   "source": [
    "Для оптимизации работы и более справедливой оценки на кросс-валидации дополнительно создадим пайплайн, который сначала будет проводить векторизацию текстов посредством функции TF-IDF, а потом обучать заданную модель. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07cdc89",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028edad6",
   "metadata": {},
   "source": [
    "Начнем с логистической регрессии. После обучения выведем среднее качество лучшей модели по cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "015e941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59a1ebe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('vectorization',\n",
       "                                              TfidfVectorizer(stop_words={'a',\n",
       "                                                                          'about',\n",
       "                                                                          'above',\n",
       "                                                                          'after',\n",
       "                                                                          'again',\n",
       "                                                                          'against',\n",
       "                                                                          'ain',\n",
       "                                                                          'all',\n",
       "                                                                          'am',\n",
       "                                                                          'an',\n",
       "                                                                          'and',\n",
       "                                                                          'any',\n",
       "                                                                          'are',\n",
       "                                                                          'aren',\n",
       "                                                                          \"aren't\",\n",
       "                                                                          'as',\n",
       "                                                                          'at',\n",
       "                                                                          'be',\n",
       "                                                                          'because',\n",
       "                                                                          'been',\n",
       "                                                                          'before',\n",
       "                                                                          'being',\n",
       "                                                                          'below',\n",
       "                                                                          'between',\n",
       "                                                                          'both',\n",
       "                                                                          'but',\n",
       "                                                                          'by',\n",
       "                                                                          'can',\n",
       "                                                                          'couldn',\n",
       "                                                                          \"couldn't\", ...})),\n",
       "                                             ('logit',\n",
       "                                              LogisticRegression(class_weight='balanced',\n",
       "                                                                 random_state=9))]),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'logit__C': array([1.00000000e-05, 8.48342898e-05, 7.19685673e-04, 6.10540230e-03,\n",
       "       5.17947468e-02, 4.39397056e-01, 3.72759372e+00, 3.16227766e+01,\n",
       "       2.68269580e+02, 2.27584593e+03, 1.93069773e+04, 1.63789371e+05,\n",
       "       1.38949549e+06, 1.17876863e+07, 1.00000000e+08]),\n",
       "                                        'logit__penalty': ['l2']},\n",
       "                   scoring='f1')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(random_state=9, class_weight='balanced')\n",
    "params_log = {'logit__C': np.logspace(-5, 8, 15),'logit__penalty':['l2']}\n",
    "pipeline =  Pipeline([(\"vectorization\", count_tf_idf), (\"logit\", logit)])\n",
    "cv_log = RandomizedSearchCV(pipeline, param_distributions=params_log,\n",
    "                      cv=3, scoring='f1', n_jobs=-1)\n",
    "cv_log.fit(X_train['text'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b2f523d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7537072044594503"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_log.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e1ad00",
   "metadata": {},
   "source": [
    "Модель логистической регрессии показала неплохое качество на тренировочных данных, а также сумела пересечь порог по необходимому качеству - хоть и по нижней границе. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5b574a",
   "metadata": {},
   "source": [
    "### Случайный лес"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d450bd",
   "metadata": {},
   "source": [
    "Теперь повторим процедуру подбора параметров для случайного леса. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7025dfaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('vectorization',\n",
       "                                              TfidfVectorizer(stop_words={'a',\n",
       "                                                                          'about',\n",
       "                                                                          'above',\n",
       "                                                                          'after',\n",
       "                                                                          'again',\n",
       "                                                                          'against',\n",
       "                                                                          'ain',\n",
       "                                                                          'all',\n",
       "                                                                          'am',\n",
       "                                                                          'an',\n",
       "                                                                          'and',\n",
       "                                                                          'any',\n",
       "                                                                          'are',\n",
       "                                                                          'aren',\n",
       "                                                                          \"aren't\",\n",
       "                                                                          'as',\n",
       "                                                                          'at',\n",
       "                                                                          'be',\n",
       "                                                                          'because',\n",
       "                                                                          'been',\n",
       "                                                                          'before',\n",
       "                                                                          'being',\n",
       "                                                                          'below',\n",
       "                                                                          'between',\n",
       "                                                                          'both',\n",
       "                                                                          'but',\n",
       "                                                                          'by',\n",
       "                                                                          'can',\n",
       "                                                                          'couldn',\n",
       "                                                                          \"couldn't\", ...})),\n",
       "                                             ('random_forest',\n",
       "                                              RandomForestClassifier(class_weight='balanced',\n",
       "                                                                     n_jobs=-1,\n",
       "                                                                     random_state=9))]),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'random_forest__max_depth': range(1, 7),\n",
       "                                        'random_forest__max_features': ['auto',\n",
       "                                                                        'sqrt',\n",
       "                                                                        'log2'],\n",
       "                                        'random_forest__n_estimators': range(90, 120, 10)},\n",
       "                   scoring='f1')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestClassifier(random_state=9, n_jobs=-1, class_weight='balanced')\n",
    "params_forest = {'random_forest__max_depth': range (1, 7, 1), 'random_forest__max_features': ['auto', 'sqrt', 'log2'], \n",
    "                 'random_forest__n_estimators': range(90, 120, 10)}\n",
    "pipeline =  Pipeline([('vectorization', count_tf_idf), (\"random_forest\", forest)])\n",
    "cv_forest = RandomizedSearchCV(pipeline, param_distributions=params_forest,\n",
    "                      cv=3, scoring='f1', n_jobs=-1)\n",
    "cv_forest.fit(X_train['text'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbf3567d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34720463587716993"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_forest.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b7f9d2",
   "metadata": {},
   "source": [
    "Модель случайного леса имеет куда более низкий уровень качества, а также не пересекает заявленный заказчиком порог - данная модель исключается из дальнейшего рассмотрения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda646c",
   "metadata": {},
   "source": [
    "### CatBoost классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d35515",
   "metadata": {},
   "source": [
    "Наконец, построим модель CatBoost классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffb26e24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('vectorization',\n",
       "                                              TfidfVectorizer(stop_words={'a',\n",
       "                                                                          'about',\n",
       "                                                                          'above',\n",
       "                                                                          'after',\n",
       "                                                                          'again',\n",
       "                                                                          'against',\n",
       "                                                                          'ain',\n",
       "                                                                          'all',\n",
       "                                                                          'am',\n",
       "                                                                          'an',\n",
       "                                                                          'and',\n",
       "                                                                          'any',\n",
       "                                                                          'are',\n",
       "                                                                          'aren',\n",
       "                                                                          \"aren't\",\n",
       "                                                                          'as',\n",
       "                                                                          'at',\n",
       "                                                                          'be',\n",
       "                                                                          'because',\n",
       "                                                                          'been',\n",
       "                                                                          'before',\n",
       "                                                                          'being',\n",
       "                                                                          'below',\n",
       "                                                                          'between',\n",
       "                                                                          'both',\n",
       "                                                                          'but',\n",
       "                                                                          'by',\n",
       "                                                                          'can',\n",
       "                                                                          'couldn',\n",
       "                                                                          \"couldn't\", ...})),\n",
       "                                             ('cat',\n",
       "                                              <catboost.core.CatBoostClassifier object at 0x000001F0CF9C4130>)]),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'cat__depth': range(1, 7),\n",
       "                                        'cat__grow_policy': ['Depthwise'],\n",
       "                                        'cat__n_estimators': range(90, 120, 10)},\n",
       "                   scoring='f1')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = CatBoostClassifier(random_state=9, verbose=False)\n",
    "params_cat = {'cat__n_estimators': range(90, 120, 10),\n",
    "               'cat__depth': range (1, 7, 1), 'cat__grow_policy': ['Depthwise']}\n",
    "pipeline =  Pipeline([('vectorization', count_tf_idf), (\"cat\", cat)])\n",
    "cat_cv = RandomizedSearchCV(pipeline, params_cat, cv=3, scoring='f1', n_jobs=-1)\n",
    "cat_cv.fit(X_train['text'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "255487d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7371877958028165"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd53d97",
   "metadata": {},
   "source": [
    "Модель классификации CatBoost показала неплохой результат на тренировочных данных, однако она не смогла пересечь порог качества, так что она также исключается из дальнейшего рассмотрения.\n",
    "\n",
    "По итогу мы остановились на модели логистической регрессии - она не только была единственной моделью, которой удалось пересечь минимальный порог качества, но также имела наибольшее значение f1 статистики на тренировочных данных. Оценим, как будет работать наша итоговая модель на тренировочных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d30582f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7508926119198024"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, cv_log.predict(X_test['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879d5d3",
   "metadata": {},
   "source": [
    "Наша итоговая модель отлично показала себя и на тренировочных данных - значение f1 статистики почти не изменилось, что свидетельствует о минимальном переобучении нашей модели. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94545c08",
   "metadata": {},
   "source": [
    "## Итоговые выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeace5da",
   "metadata": {},
   "source": [
    "Перед нами стояла задача создать модель, которая по предлагаемому ей тексту будет определять, является ли этот текст токсичным или нет. \n",
    "\n",
    "На первом этапе работы мы провели первичную обработку исходных текстов:\n",
    "* перевели тексты в нужную кодировку\n",
    "* лемматизировали исходные тексты\n",
    "* удалили ненужные и неинформативные слова\n",
    "* почистили  тексты от стоп-слов\n",
    "* провели оценку значимости каждого слова в тексте\n",
    "\n",
    "После этапа предобработки мы перешли к этапу моделирования. Нами были рассмотрены три модели: логистическая регрессия, случайный лес и CatBoost классификатор. В качестве итоговой была выбрана модель логистической регрессии - именно она имела наилучшее качество на тренировочных данных, а также соответствовала запросу заказчика в f1-score не ниже 0,75. \n",
    "\n",
    "На тестовых данных итоговая модель показала себя отлично - величина f1 статистики почти не отличается от аналогичного показателя на тренировочных данных, что свидетельствует о минимальном переобучении итоговой модели. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
